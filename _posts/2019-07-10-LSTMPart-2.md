---
layout: post
title:  "RNN Series:LSTM internals:Part-2:The Forward pass"
excerpt: "<strong>In this multi-part series we look inside LSTM forward pass.</strong> Read the <a href='/articles/2019-07/LSTMPart-1'>part-1's</a> before you come back here. Once you are back, in this article, we explore the meaning, math and the implementation of an LSTM cell. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>"
mathjax: true
date:   2019-07-09 15:07:19
categories: [article]
tags: Artificial-Intelligence Deep-Learning LSTM RNN Sequence-Modelling
comments: true
---

### Introduction
<strong>In this multi-part series we look inside LSTM forward pass.</strong> Read the <a href='/articles/2019-07/LSTMPart-1'>part-1's</a> before you come back here. Once you are back, in this article, we explore the meaning, math and the implementation of an LSTM cell. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>

### What this article is not about?
> * This article will not talk about the conceptual model of LSTM, on which there are some great existing material [here][lstm-1] and [here][lstm-2] in thye order of difficulty.
> * This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome post by Andrej if somewhat [difficult post][lstm-3].
> * This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts [here][lstm-4] and [here][lstm-5] in the order of difficulty     

### What this article is about?
> * Using the [first principles][first principles] we picturize the forward pass of an LSTM cell.
> * Then we associate code with picture to make our understanding more concrete  

### Context
This is the same example and the context is the same as described in [part-1]. <strong>The focus however, this time is on LSTM cell.</strong>

### LSTM Cell
While there are many variations to the LSTM cell. In the current article we are looking at [LayerNormBasicLSTMCell].
> * Weights and biases of the LSTM cell.
> * Shapes color coded.
{%
    include image.html
    src="/img/rnn/lstm1.jpg"
    caption="figure-1: Batch=1, State=5(internal states(C and H)), input_size=10(assuming vocab size of 10 translates to one-hot of size 10) ."
    hight="110%"
    width="110%"
%}

> * Weights, biases of the LSTM cell. Also shown is the cell state (c-state) over and above the h-state of a vanilla RNN.
> * h<sub>t-1</sub> may be initialized by default with zeros.
{%
    include image.html
    src="/img/rnn/lstm2.jpg"
    caption="figure-2: Weights and biases for all the gates f(forget), i(input), c(candidatecellstate), o(output). Shape of these <strong>weights</strong> are <strong>5X15</strong> individually and stacked vertically as shown they measure <strong>20X15</strong>. Shape of the <strong>biases</strong> are <strong>5X1</strong> individually and stacked up <strong>20X1</strong>."
    hight="110%"
    width="110%"
%}

> * Weights and biases in the [code][code-1].
> * Often the code accepts constants as weights for comparision with [Tensorflow]  
{%
    include image.html
    src="/img/rnn/lstm3.jpg"
    caption="figure-3: <strong>4*hidden_size(5),input_size(10)+hidden_size(5)+1(biases)</strong>. Making it <strong>20X16</strong>, biases allocated with weights is a matter of convenience and performance."
    hight="110%"
    width="110%"
%}

> * Equations of LSTM and their interplay with weights and biases.
> * h<sub>t-1</sub> and c<sub>t-1</sub> may be initialized by default with zeros(Language Model) or some non-zero state for conditional language model.
{%
    include image.html
    src="/img/rnn/lstm4.jpg"
    caption="figure-4: Equations of LSTM and their interplay with weights and biases."
    hight="110%"
    width="110%"
%}

> * GEMM(General Matrix Multiplication) in [code][code-2]
> * h and x concatenation is only a performance convenience.
{%
    include image.html
    src="/img/rnn/lstm5.jpg"
    caption="figure-5: GEMM and running them though various gates."
    hight="110%"
    width="110%"
%}

> * The <strong>mathematical reason</strong> why <strong>vanishing gradient</strong> is <strong>mitigated</strong>.
> * There are many theories though. Look into this in detail with back propagation.
{%
    include image.html
    src="/img/rnn/lstm6.jpg"
    caption="figure-6: New cell state is a function of old cell state and new candidate state."
    hight="110%"
    width="110%"
%}

> * New cell state(c-state) in [code][code-3]
> * This is used in 2 ways as illustrated in figure-8.
{%
    include image.html
    src="/img/rnn/lstm7.jpg"
    caption="figure-7: Batch enables parallelism, but for simplicity assumed as one."
    hight="110%"
    width="110%"
%}

> * c-state<sub>t</sub> is used to calculate the "externally visible" h-state<sub>t</sub>
> * Sometimes referred to as c<sub>t</sub>, h<sub>t</sub>.
{%
    include image.html
    src="/img/rnn/lstm8.jpg"
    caption="figure-8: Conceptually it is the same cell transitioning into the next state. In this case c<sub>t</sub>."
    hight="110%"
    width="110%"
%}

> * New h-state in [code][code-4]
> * The transition to the next time step is complete with h<sub>t</sub>.
{%
    include image.html
    src="/img/rnn/lstm9.jpg"
    caption="figure-9: Conceptually it is the same cell transitioning into the next state. In this case h<sub>t</sub>.  
    hight="110%"
    width="110%"
%}

> * Sequence of 3 shown below, but that can depend on the use-case.
{%
    include image.html
    src="/img/rnn/lstm10.jpg"
    caption="figure-10: Batch enables parallelism, but for simplicity assumed as one."
    hight="110%"
    width="110%"
%}

> * Sequence of 3 shown below, but that can depend on the use-case.
{%
    include image.html
    src="/img/rnn/lstm11.jpg"
    caption="figure-2: Batch enables parallelism, but for simplicity assumed as one."
    hight="110%"
    width="110%"
%}

> * Sequence of 3 shown below, but that can depend on the use-case.
{%
    include image.html
    src="/img/rnn/lstm12.jpg"
    caption="figure-2: Batch enables parallelism, but for simplicity assumed as one."
    hight="110%"
    width="110%"
%}

> * Sequence of 3 shown below, but that can depend on the use-case.
{%
    include image.html
    src="/img/rnn/lstm13.jpg"
    caption="figure-2: Batch enables parallelism, but for simplicity assumed as one."
    hight="110%"
    width="110%"
%}

> * Sequence of 3 shown below, but that can depend on the use-case.
{%
    include image.html
    src="/img/rnn/lstm14.jpg"
    caption="figure-2: Batch enables parallelism, but for simplicity assumed as one."
    hight="110%"
    width="110%"
%}


### Summary
In summary then, that was the walk though of code surrounding LSTM training, without getting into LSTM or its weights just yet. That would be the topic of our next article.    

[part-1]: /articles/2019-07/LSTMPart-1
[part-2]: /articles/2019-07/LSTMPart-2
[first principles]: https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0
[Deep-Breathe]: https://github.com/slowbreathing/Deep-Breathe
[LayerNormBasicLSTMCell]: https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell
[lstm-2]: https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
[lstm-3]: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
[lstm-4]: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html
[lstm-5]: https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577

[code-1]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L76
[code-2]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L210-L214
[code-3]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L215
[code-4]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L216
[softmax]: /articles/2019-05/softmax-and-its-gradient
[cross_entropy_loss]: /articles/2019-05/softmax-and-cross-entropy
[Tensorflow]: https://www.tensorflow.org/
[Listing-1]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/tfwordslstm.py
[Listing-2]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMainGraph.py
[Listing-3]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMain.py
[scr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L13
[scr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L15
[scr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L14
[pygr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn.py#L75-L76
[pygr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/common.py#L241-L255
[pygr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn_cell.py#L79-L81
[pygr-4]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/core.py#L107-L112
