<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="LSTMs make up everything directly or indirectly from Time Series(TS) to Neural Machine Translators(NMT) to Bidirectional Encoder Representation from Transfor...">
  <meta name="keywords" content="blog and jekyll">
  <meta name="author" content="RNN Series:LSTM internals:Part-1:The Big Picture | Slowbreathing">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="RNN Series:LSTM internals:Part-1:The Big Picture | Slowbreathing">
  <meta name="twitter:description" content="LSTMs make up everything directly or indirectly from Time Series(TS) to Neural Machine Translators(NMT) to Bidirectional Encoder Representation from Transfor...">
  
    <meta property="twitter:image" content="http://localhost:4000/img/leonids-logo.png">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="http://localhost:4000/articles/2019-07/LSTMPart-1">
  <meta property="og:title" content="RNN Series:LSTM internals:Part-1:The Big Picture | Slowbreathing">
  <meta property="og:description" content="LSTMs make up everything directly or indirectly from Time Series(TS) to Neural Machine Translators(NMT) to Bidirectional Encoder Representation from Transfor...">
  
    <meta property="og:image" content="http://localhost:4000/img/leonids-logo.png">
  
  <title>RNN Series:LSTM internals:Part-1:The Big Picture | Slowbreathing</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="http://localhost:4000/css/font-awesome.min.css">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">

  <link rel="canonical" href="http://localhost:4000/articles/2019-07/LSTMPart-1">
  <link rel="alternate" type="application/rss+xml" title="Slowbreathing" href="http://localhost:4000/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.png">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142206738-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-142206738-1');
  </script>-->

</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="http://localhost:4000/">
    <img src="http://localhost:4000/img/leonids-logo.png" alt="" class="avatar">
  </a>
  
  <a href="http://localhost:4000/" class="author_name">Mohit Kumar</a>
  <span class="author_job">Researcher/Consultant/Trainer</span>
  <span class="author_bio mbm">Programming is more than just typing.</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="http://localhost:4000/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="http://localhost:4000/archive/">Archive</a>
      </li>
          
      <li class="nav-item">
        <a href="http://localhost:4000/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="http://localhost:4000/resume/">about me</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/tags/">Tags</a>
      </li>
         
    </ul>
  </nav>
  <script type="text/javascript">
  // based on https://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
      <li>
      <script>gen_mail_to_link('mohit.riverstone@gmail.com', 'Hello from website');</script>
      </li>
    
    
    <li><a href="https://facebook.com/mohit.kumar.965" class="social-link-item" target="_blank"><i class="fa fa-fw fa-facebook"></i></a></li>
    
    <li><a href="https://linkedin.com/in/mohit-kumar-36050a65" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    
    
    <li><a href="https://github.com/Slowbreathing" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <a class="btn" href= "http://localhost:4000/" >
  Home
</a>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>





<div id="post">
  <header class="post-header">
    <h1 title="RNN Series:LSTM internals:Part-1:The Big Picture">RNN Series:LSTM internals:Part-1:The Big Picture</h1>
    <span class="post-meta">
      <span class="post-date">
        9 JUL 2019
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    4 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <h3 id="introduction">Introduction</h3>
<p>LSTMs today are cool. Everything, well almost everything, in the modern Deep Learning landscape is made of LSTMs. I know that might be an exaggeration but, it can only be understated that they are absolutely indispensable. They make up everything directly or indirectly from Time Series(TS) to Neural Machine Translators(NMT) to Bidirectional Encoder Representation from Transformers(BERT) to Neural Turing Machine(NTM) to Differentiable Neural Computers(DNC) etc, and yet they are not very well understood. Whilst there are the elites that write frameworks (Tensorflow, Pytorch, etc), but they are not surprisingly very few in number. The common “Machine learning/Deep Learning”  “man/programmer” at best know how to use it. <strong>This is a multi-part series that will unravel the mystery behind LSTMs. Especially it’s gradient calculation when participating in a more complex model like NMT,BERT,NTM,DNC, etc. I do this using the <a href="https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0">first principles</a> approach for which I have pure python implementation (<a href="https://github.com/slowbreathing/Deep-Breathe">Deep-Breathe</a>) of most complex Deep Learning models.</strong></p>

<h3 id="what-this-article-is-not-about">What this article is not about?</h3>
<blockquote>
  <ul>
    <li>This article will not talk about the conceptual model of LSTM, on which there are some great existing material <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">here</a> and <a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">here</a> in thye order of difficulty.</li>
    <li>This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome post by Andrej if somewhat <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">difficult post</a>.</li>
    <li>This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts <a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html">here</a> and <a href="https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577">here</a> in the order of difficulty</li>
  </ul>
</blockquote>

<h3 id="what-this-article-is-about">What this article is about?</h3>
<blockquote>
  <ul>
    <li>The reason why understanding LSTM’s inner working is vital is because it underpins most important models of modern Artificial Intelligence.</li>
    <li>Underpins <strong>memory augmented</strong> models and architecture.</li>
    <li>Most often it is not used in its vanilla form. There is directionality involved, multiple layers involved, making it a complex moving piece inself.</li>
    <li>In this multi-part series I get under the cover of an LSTM using <a href="https://github.com/slowbreathing/Deep-Breathe">Deep-Breathe</a> especially its back propagation when it is a part of a more complex model.</li>
  </ul>
</blockquote>

<h3 id="context">Context</h3>
<p>Start with the simplest word-generation case-study. We train an LSTM network on a short story. Once trained, we ask it to generate new stories giving it a cue of a few starting words. The focus primarily is on the training process here.</p>
<blockquote>
  <ul>
    <li>Picking a starting point in the story that is randomly chosen.</li>
    <li>The training process is feeding in a sequence of n words from the story and checking if (n+1)th word predicted by the model is correct or not. Back propagating and adjusting the weights of the model.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/rnn.jpeg" alt="Image: figure-1: While there are many ways in which the RNNs can be used. For this example we consider the many-to-one organization." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-1: While there are many ways in which the RNNs can be used. For this example we consider the many-to-one organization.</span></center></figcaption>
</figure>

<h3 id="training-process">Training Process</h3>
<p>In the next few illustrations we are looking at primary the shape of input, how is the input fed in, when do we check the output before back-propagation happens, etc. All this <strong>without looking inside the LSTM Cell just yet</strong>. Batch=1(for parallelism), sequence=3(3 words in our example), size=10(vocabulary size 10 so our representation is is a one-hot of size 10).</p>

<blockquote>
  <ul>
    <li>Sequence of 3 shown below, but that can depend on the use-case.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/rnn.jpg" alt="Image: figure-2: Batch enables parallelism, but for simplicity assumed as one." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-2: Batch enables parallelism, but for simplicity assumed as one.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Functionally dynamic_rnn feeds in the batches of sequences but its code is written using “tf.while_loop” and not one of python’s native loop construct. With any one of python’s native loop construct only the code within an iteration be optimized on a GPU but with “tf.while_loop” the advantage is that the complete loop can be optimed on a GPU.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/rnn1.jpg" alt="Image: figure-3: Function logic of dynamic_rnn on the bottom right. Complete listing of &lt;a href='https://github.com/slowbreathing/Deep-Breathe'&gt;DEEP-Breathe's&lt;/a&gt; dynamic_rnn can be found at &lt;a href='https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/rnn.py'&gt;here&lt;/a&gt;." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-3: Function logic of dynamic_rnn on the bottom right. Complete listing of <a href="https://github.com/slowbreathing/Deep-Breathe">DEEP-Breathe's</a> dynamic_rnn can be found at <a href="https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/rnn.py">here</a>.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Returns a tuple of <strong>output(H-state across all time steps)</strong> and states(C-state and H-state for the lst time step).</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/rnn2.jpg" alt="Image: figure-4: &lt;strong&gt;output(H-state across all time steps)&lt;/strong&gt;" hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-4: <strong>output(H-state across all time steps)</strong></span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Returns a tuple of output(H-state across all time steps) and <strong>states(C-state and H-state for the lst time step)</strong>.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/rnn3.jpg" alt="Image: figure-5: &lt;strong&gt;states(C-state and H-state for the lst time step)&lt;/strong&gt;." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-5: <strong>states(C-state and H-state for the lst time step)</strong>.</span></center></figcaption>
</figure>

<blockquote>
  <p>*</p>
</blockquote>

<figure>
    
    <img src="/img/rnn/rnn4.jpg" alt="Image: figure-6:Cost is low because, the prediction is closer to the truth." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-6:Cost is low because, the prediction is closer to the truth.</span></center></figcaption>
</figure>

<figure>
    
    <img src="/img/rnn/rnn5.jpg" alt="Image: figure-7:Cost is low because, the prediction is closer to the truth." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-7:Cost is low because, the prediction is closer to the truth.</span></center></figcaption>
</figure>


  </article>
</div>

<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/articles/2019-07/LSTMPart-1" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/articles/2019-07/LSTMPart-1" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=http://localhost:4000/articles/2019-07/LSTMPart-1" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=http://localhost:4000/articles/2019-07/LSTMPart-1" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=http://localhost:4000/articles/2019-07/LSTMPart-1" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->



        <footer>
  <strong>MIT License</strong> &copy; 2019 Mohit Kumar.
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="http://localhost:4000/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="http://localhost:4000/js/main.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-142206738-1', 'auto');
  ga('send', 'pageview');
</script>



</body>
</html>
