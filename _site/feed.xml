<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slowbreathing</title>
    <description>Programming is more than just typing.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>2019-07-11</pubDate>
    <lastBuildDate>Thu, 11 Jul 2019 08:00:15 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>RNN Series:LSTM internals:Part-1:The Big Picture</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;LSTMs today are cool. Everything, well almost everything, in the modern Deep Learning landscape is made of LSTMs. I know that might be an exaggeration but, it can only be understated that they are absolutely indispensable. They make up everything directly or indirectly from Time Series(TS) to Neural Machine Translators(NMT) to Bidirectional Encoder Representation from Transformers(BERT) to Neural Turing Machine(NTM) to Differentiable Neural Computers(DNC) etc, and yet they are not very well understood. Whilst there are the elites that write frameworks (Tensorflow, Pytorch, etc), but they are not surprisingly very few in number. The common “Machine learning/Deep Learning”  “man/programmer” at best know how to use it. &lt;strong&gt;This is a multi-part series that will unravel the mystery behind LSTMs. Especially it’s gradient calculation when participating in a more complex model like NMT,BERT,NTM,DNC, etc. I do this using the &lt;a href=&quot;https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0&quot;&gt;first principles&lt;/a&gt; approach for which I have pure python implementation (&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;Deep-Breathe&lt;/a&gt;) of most complex Deep Learning models.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-this-article-is-not-about&quot;&gt;What this article is not about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This article will not talk about the conceptual model of LSTM, on which there are some great existing material &lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html&quot;&gt;here&lt;/a&gt; in thye order of difficulty.&lt;/li&gt;
    &lt;li&gt;This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome post by Andrej if somewhat &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;difficult post&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts &lt;a href=&quot;https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577&quot;&gt;here&lt;/a&gt; in the order of difficulty&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;what-this-article-is-about&quot;&gt;What this article is about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The reason why understanding LSTM’s inner working is vital is because it underpins most important models of modern Artificial Intelligence.&lt;/li&gt;
    &lt;li&gt;Underpins &lt;strong&gt;memory augmented&lt;/strong&gt; models and architecture.&lt;/li&gt;
    &lt;li&gt;Most often it is not used in its vanilla form. There is directionality involved, multiple layers involved, making it a complex moving piece inself.&lt;/li&gt;
    &lt;li&gt;In this multi-part series I get under the cover of an LSTM using &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;Deep-Breathe&lt;/a&gt; especially its back propagation when it is a part of a more complex model.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;
&lt;p&gt;Start with the simplest word-generation case-study. We train an LSTM network on a short story. Once trained, we ask it to generate new stories giving it a cue of a few starting words. The focus primarily is on the training process here.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Picking a starting point in the story that is randomly chosen.&lt;/li&gt;
    &lt;li&gt;The training process is feeding in a sequence of n words from the story and checking if (n+1)th word predicted by the model is correct or not. Back propagating and adjusting the weights of the model.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn.jpeg&quot; alt=&quot;Image: figure-1: While there are many ways in which the RNNs can be used. For this example we consider the many-to-one organization.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: While there are many ways in which the RNNs can be used. For this example we consider the many-to-one organization.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;training-process&quot;&gt;Training Process&lt;/h3&gt;
&lt;p&gt;In the next few illustrations we are looking at primary the shape of input, how is the input fed in, when do we check the output before back-propagation happens, etc. All this &lt;strong&gt;without looking inside the LSTM Cell just yet&lt;/strong&gt;. Batch=1(for parallelism), sequence=3(3 words in our example), size=10(vocabulary size 10 so our representation is is a one-hot of size 10).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Sequence of 3 shown below, but that can depend on the use-case.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn.jpg&quot; alt=&quot;Image: figure-2: Batch enables parallelism, but for simplicity assumed as one.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: Batch enables parallelism, but for simplicity assumed as one.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Functionally dynamic_rnn feeds in the batches of sequences but its code is written using “tf.while_loop” and not one of python’s native loop construct. With any one of python’s native loop construct only the code within an iteration be optimized on a GPU but with “tf.while_loop” the advantage is that the complete loop can be optimed on a GPU.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn1.jpg&quot; alt=&quot;Image: figure-3: Function logic of dynamic_rnn on the bottom right. Complete listing of &amp;lt;a href='https://github.com/slowbreathing/Deep-Breathe'&amp;gt;DEEP-Breathe's&amp;lt;/a&amp;gt; dynamic_rnn can be found at &amp;lt;a href='https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/rnn.py'&amp;gt;here&amp;lt;/a&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: Function logic of dynamic_rnn on the bottom right. Complete listing of &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;DEEP-Breathe's&lt;/a&gt; dynamic_rnn can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/rnn.py&quot;&gt;here&lt;/a&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Returns a tuple of &lt;strong&gt;output(H-state across all time steps)&lt;/strong&gt; and states(C-state and H-state for the lst time step).&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn2.jpg&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;output(H-state across all time steps)&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;output(H-state across all time steps)&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Returns a tuple of output(H-state across all time steps) and &lt;strong&gt;states(C-state and H-state for the lst time step)&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn3.jpg&quot; alt=&quot;Image: figure-5: &amp;lt;strong&amp;gt;states(C-state and H-state for the lst time step)&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: &lt;strong&gt;states(C-state and H-state for the lst time step)&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Multiplier Wy and By which among other things shape last h into shape of X(10) so a comparision can be made.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn4.jpg&quot; alt=&quot;Image: figure-6: So the shape of &amp;lt;strong&amp;gt;pred&amp;lt;/strong&amp;gt;(the first return from 'RNN()' function should be that of &amp;lt;strong&amp;gt;X&amp;lt;/strong&amp;gt;)&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: So the shape of &lt;strong&gt;pred&lt;/strong&gt;(the first return from 'RNN()' function should be that of &lt;strong&gt;X&lt;/strong&gt;)&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Finally &lt;a href=&quot;/articles/2019-05/softmax-and-its-gradient&quot;&gt;softmax&lt;/a&gt; and a [cross_entropy_loss][cross_entropy_loss].&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn5.jpg&quot; alt=&quot;Image: figure-7:Cost is low because, the prediction is closer to the truth.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7:Cost is low because, the prediction is closer to the truth.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

</description>
        <pubDate>2019-07-11</pubDate>
        <link>http://localhost:4000/articles/2019-07/LSTMPart-1</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2019-07/LSTMPart-1</guid>
        
        <category>Artificial-Intelligence</category>
        
        <category>Deep-Learning</category>
        
        <category>LSTM</category>
        
        <category>RNN</category>
        
        <category>Sequence-Modelling</category>
        
        
        <category>article</category>
        
      </item>
    
      <item>
        <title>Softmax and Cross-entropy</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and predicted value(yhat). Cross entropy is a loss function that is defined as &lt;script type=&quot;math/tex&quot;&gt;\Large E = -y .log ({\hat{Y}})&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\Large E&lt;/script&gt;, is defined as error, &lt;script type=&quot;math/tex&quot;&gt;\Large y&lt;/script&gt; is the label and &lt;script type=&quot;math/tex&quot;&gt;\Large \hat{Y}&lt;/script&gt; is defined as &lt;script type=&quot;math/tex&quot;&gt;\Large softmax_j (logits)&lt;/script&gt; and logits are weighted sum. One of the reason to choose cross entropy alongside softmax is that because softmax has an exponential element inside it the, an element log provides for convex cost function. This is similar to logistic regression which uses sigmoid. Mathematically expressed as below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \hat{Y}=softmax_j (logits)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large E = -y .log ({\hat{Y}})&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Large y&lt;/script&gt; is the label and Yhat &lt;script type=&quot;math/tex&quot;&gt;\Large \hat{Y}&lt;/script&gt; the predicted value.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
  One at a time.
  args:
      pred-(seq=1),input_size
      labels-(seq=1),input_size
  &quot;&quot;&quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Above is a &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/common.py&quot;&gt;loss&lt;/a&gt; implementation.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;c&quot;&gt;#prints out 0.145&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#prints out 17.01&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As you can see the loss function accepts softmaxed input and one-hot encoded labels.
The output is illustated in figure-1 and 2 below.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Loss function: Example-1&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/cel.jpg&quot; alt=&quot;Image: figure-1:Cost is low because, the prediction is closer to the truth.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1:Cost is low because, the prediction is closer to the truth.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Loss function: Example-2&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/cel2.jpg&quot; alt=&quot;Image: figure-2:Cost is high because, the prediction is far away from the truth.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2:Cost is high because, the prediction is far away from the truth.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;crossentropyloss&quot;&gt;CrossEntropyLoss&lt;/h3&gt;
&lt;p&gt;CrossEntropyLoss Function is the same loss function above but simplified and adapted for calculating the loss for multiple time steps as is usually required in RNNs. Infact it calls the same loss function internally. In the above example we are making 2 comparisions because we are passing 2 sets of logits(x) and 2 labels(y). The labels further have to be adapted into a one-hot of 4 so that they can be compared. Assuming that the abobe 2 comparisions are for 2 time timesteps, the abobe results can be achieved by calling the the &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/common.py&quot;&gt;CrossEntropyLoss&lt;/a&gt; function that calculates the softmax internally.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Does an internal softmax before loss calculation.
    args:
        pred- batch,seq,input_size
        labels-batch,seq(has to be transformed before comparision with preds(line-133).)
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;checkdatadim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;checkdatadim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;yhat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seqnum&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;yhat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seqnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seqnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lossesa&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seqnum&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;lossesa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seqnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yhat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seqnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seqnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yhat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lossesa&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The point to keep in mind is, it accepts it’s 2 inputs in 3(batch,seq,input_size) and 2(batch,seq) dimensions respectively. Batch size usually indicates multiple parallel input sequences, can be ignored for now and be assumed as 1. The shape of pred in our case is batch=1,seq=2,input_size=4. And the shape of labels is batch=1, seq=2. This is illustrated in Listing-3 and Listing-4&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

  &lt;span class=&quot;c&quot;&gt;#prints array([[ 0.14507794, 17.01904505]]))&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;softmaxed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;loss:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As illustrated in Listing-3 and Listing-4 &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;Deep-Breathe&lt;/a&gt; version of cross_entropy_loss function returns a tuple of softmaxed output that it calculates internally(only for convenience) and the Loss. The calculated loss, as expected, is the same as before as was while calling the loss function directly.&lt;/p&gt;

&lt;h3 id=&quot;crossentropyloss-derivative&quot;&gt;CrossEntropyLoss Derivative&lt;/h3&gt;
&lt;p&gt;One of the tricks I have learnt to get back propagation right is to write the equations backwards. This becomes especially useful when the model is more complex in later articles. A trick that I use a lot.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \hat{Y}=softmax_j (logits)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large E = -y .log ({\hat{Y}})&lt;/script&gt;

&lt;p&gt;The above equation is writen like so below while calculating the gradients.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large E = -y .log ({\hat{Y}})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \hat{Y}=softmax_j (logits)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Gradient: Example-3&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/cel3.jpg&quot; alt=&quot;Image: figure-3:The red arrow follows the gradient.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3:The red arrow follows the gradient.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Deriving the gradients now.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = \frac{\partial {E}}{\partial {\hat{Y}}}.\frac{\partial {\hat{Y}}}{\partial {logits}}&lt;/script&gt;

&lt;h1&gt;&lt;a name=&quot;eq-2&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\frac{\partial {\hat{Y}}}{\partial {logits}} \:\:\:\: eq(2)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;For calculating &lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial {\hat{Y}}}{\partial {logits}}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\Large \hat{y}&lt;/script&gt; is the softmax and its derivative is given by &lt;a href=&quot;softmax-and-its-gradient#eq-1&quot;&gt;eq-1&lt;/a&gt;. Combining &lt;a href=&quot;softmax-and-its-gradient#eq-1&quot;&gt;eq-1&lt;/a&gt; and &lt;a href=&quot;softmax-and-cross-entropy#eq-2&quot;&gt;eq-2&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\begin{Bmatrix}
softmax_i(1-softmax_j) &amp; i= j \\
{-softmax_j}.{softmax_i} &amp; i\neq j
\end{Bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\hat{y_t}.(1-\hat{y_t}) + \sum_{i\neq j}(\frac{-y_t}{\hat{y_t}})(-\hat{y_t}\hat{y_t})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = -y.(1-\hat{y_t}) + \sum_{i\neq j}y_t.\hat{y_t}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = -y+y.\hat{y_t} + \sum_{i\neq j}y_t.\hat{y_t}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = y.\hat{y_t} + \sum_{i\neq j}y_t.\hat{y_t} -y&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = \hat{y_t}.(y + \sum_{i\neq j}y_t) -y&lt;/script&gt;

&lt;p&gt;Since Y is a one hot vector, the term “&lt;script type=&quot;math/tex&quot;&gt;\Large (y + \sum_{i\neq j}y_t)&lt;/script&gt;” sums up to one.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = (\hat{y_t} -y) \:\:\:\: eq(3)&lt;/script&gt;

&lt;h3 id=&quot;crossentropyloss-derivative-implementation&quot;&gt;CrossEntropyLoss Derivative implementation&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;  &lt;span class=&quot;n&quot;&gt;softmaxed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;loss:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;target_one_hot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Adapting the shape of Y&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;target_one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmaxed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_one_hot&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# prints out gradient: [[[ 2.14400878e-03  1.58422012e-02  1.17058913e-01 -1.35045123e-01]&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#                        [ 8.94679461e-04 -9.99999959e-01  1.79701173e-02  9.81135163e-01]]]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gradient:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>2019-07-11</pubDate>
        <link>http://localhost:4000/articles/2019-05/softmax-and-cross-entropy</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2019-05/softmax-and-cross-entropy</guid>
        
        <category>Artificial-Intelligence</category>
        
        <category>Deep-Learning</category>
        
        
        <category>article</category>
        
      </item>
    
      <item>
        <title>Softmax and its Gradient</title>
        <description>&lt;p&gt;From the perspective of deep neural network, softmax is one the most important activation function, maybe the most important. I had trouble understanding it in the beginning, especially its why its chosen, its gradient, its relationship with cross entropy loss and the combined gradient.&lt;br /&gt;
There are many softmax resources available in the internet. Among the many that are availble, I found the link &lt;a href=&quot;https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/&quot;&gt;here&lt;/a&gt; to be the most complete. In this article, I “dumb” it down further and add code to the theory. Code when associated with theory makes the explanation very precise. &lt;strong&gt;But, there is another selfish reason for reproducing these here, I will be able to refer to these while explaining more complex concepts like LSTMs, NMT, BERT etc.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Softmax is essentially a vector function. It takes n inputs and produces and n outputs. &lt;strong&gt;The out can be interpreted as a probabilistic output(summing up to 1). A multiway shootout if you will.&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large softmax(a)=\begin{bmatrix}
a_1\\
a_2\\
\cdots\\
a_N
\end{bmatrix}\rightarrow \begin{bmatrix}
S_1\\
S_2\\
\cdots\\
S_N
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;And the actual per-element formula is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large softmax_j = \frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}&lt;/script&gt;

&lt;p&gt;As one can see the output function can only be positive because of the exponential and the values range between 0 and 1. Also,  as the value  appears in the denominator summed up with other positive numbers.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#row represents num classes but they may be real numbers&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#So the shape of input is important&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#([[1, 3, 5, 7],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#  [1,-9, 4, 8]]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;#softmax will be for each of the 2 rows&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[[2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[8.94679461e-04 4.06183847e-08 1.79701173e-02 9.81135163e-01]]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#respectively But if the input is Tranposed clearly the answer&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#will be wrong.&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;#That needs to be converted to probability&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#column represents the vocabulary size.&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predsl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#print(&quot;inputs:&quot;,inputs)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;predsl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predsl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Above is a &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/common.py&quot;&gt;softmax&lt;/a&gt; implementation.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;softmax:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#prints out&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[[2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[8.94679461e-04 4.06183847e-08 1.79701173e-02 9.81135163e-01]]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Above is a &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/softmaxtest.py&quot;&gt;softmaxtest&lt;/a&gt; implementation.&lt;/p&gt;

&lt;p&gt;The softmax function is very similar to the Logistic regression cost function. The only difference being that the sigmoid makes the output binary interpretable whereas, softmax’s output can be interpreted as a multiway shootout. With the the above two rows individually summing up to one.&lt;/p&gt;

&lt;h3 id=&quot;softmax-derivative&quot;&gt;Softmax Derivative&lt;/h3&gt;
&lt;p&gt;Before diving into computing the derivative of softmax, let’s start with some preliminaries from vector calculus.&lt;/p&gt;

&lt;p&gt;Softmax is fundamentally a vector function. It takes a vector as input and produces a vector as output; in other words, it has multiple inputs and multiple outputs. Therefore, we cannot just ask for “the derivative of softmax”; We should instead specify:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Which component (output element) of softmax we’re seeking to find the derivative of.&lt;/li&gt;
  &lt;li&gt;Since softmax has multiple inputs, with respect to which input element the partial derivative is computed.
This is exactly why the notation of vector calculus was developed. What we’re looking for is the partial derivatives:&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial softmax_i}{\partial a_j}&lt;/script&gt;

&lt;p&gt;This is the partial derivative of the i-th output w.r.t. the j-th input. A shorter way to write it that we’ll be using going forward is:
Since softmax is a  function, the most general derivative we compute for it is the Jacobian matrix:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large Dsoftmax=\begin{bmatrix}
D_1 softmax_1\:\:\:\:\:  \cdots\:\:\:\:\: D_N softmax_1 \\
\vdots\:\:\:\:\: \ddots\:\:\:\:\: \vdots \\
D_1 softmax_N\:\:\:\:\: \cdots\:\:\:\:\: D_N softmax_N
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;Let’s compute &lt;script type=&quot;math/tex&quot;&gt;D_jsoftmax_i&lt;/script&gt; for arbitrary i and j:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\large D_jsoftmax_i=\Large \:\:\frac{\partial softmax_i}{\partial a_j}\:\:= \frac{\partial \frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}}{\partial a_j}&lt;/script&gt;&lt;br /&gt;
Using the quotient rule &lt;script type=&quot;math/tex&quot;&gt;\Large f(x) = \frac{g(x)}{h(x)}&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;\Large f'(x) = \frac{g'(x)h(x) - h'(x)g(x)}{h(x)^2}&lt;/script&gt; &lt;br /&gt;
in our case &lt;script type=&quot;math/tex&quot;&gt;\Large g_i = e^{a_i}&lt;/script&gt;&lt;br /&gt;
                   &lt;script type=&quot;math/tex&quot;&gt;\Large h_i = \sum_{k=1}^{N} e^{a_k}&lt;/script&gt; simplifying &lt;script type=&quot;math/tex&quot;&gt;\Large \sum&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;\Large \sum_{k=1}^{N} e^{a_k}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Note that no matter which &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; we compute the derivative of &lt;script type=&quot;math/tex&quot;&gt;h_i&lt;/script&gt; for, the answer will always be &lt;script type=&quot;math/tex&quot;&gt;e^{a_j}&lt;/script&gt;. This is not the case for &lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt;, howewer. The derivative of &lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt; w.r.t. &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;e^{a_j}&lt;/script&gt; only if i=j, because only then &lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt; has &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; anywhere in it. Otherwise, the derivative is 0.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Going back to our &lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial softmax_i}{\partial a_j}&lt;/script&gt; we’ll start with the &lt;script type=&quot;math/tex&quot;&gt;\Large i=j&lt;/script&gt; case. &lt;br /&gt;
Then, using the quotient rule we have:
&lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial\frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}}{\partial a_j} = \frac{e^{a_i}.\sum-e^{a_j}.e^{a_i}}{\sum^2}&lt;/script&gt;&lt;br /&gt;
                         &lt;script type=&quot;math/tex&quot;&gt;\Large = \frac{e^{a_i}}{\sum}.\frac{\sum-e^{a_j}}{\sum}&lt;/script&gt;&lt;br /&gt;
                         &lt;script type=&quot;math/tex&quot;&gt;\Large = softmax_i(1-softmax_j)&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;similarly for &lt;script type=&quot;math/tex&quot;&gt;\Large i \neq j&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial\frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}}{\partial a_j} = \frac{0- e^{a_j}e^{a_i}}{\sum^2}&lt;/script&gt;&lt;br /&gt;
                         &lt;script type=&quot;math/tex&quot;&gt;\Large = \frac{- e^{a_j}}{\sum}.\frac{e^{a_i}}{\sum}&lt;/script&gt;&lt;br /&gt;
                         &lt;script type=&quot;math/tex&quot;&gt;\Large = {-softmax_j}.{softmax_i}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To summarize:
&lt;a name=&quot;eq-1&quot;&gt;&lt;/a&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\Large D_jsoftmax_i=\begin{Bmatrix}
softmax_i(1-softmax_j) &amp; i= j \\
{-softmax_j}.{softmax_i} &amp; i\neq j
\end{Bmatrix} \:\:\:\: eq(1) %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;softmax-derivative-implementation&quot;&gt;Softmax Derivative Implementation&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_softmax_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Below is the softmax value for [1, 3, 5, 7]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# [2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# initialize the 2-D jacobian matrix.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;equal:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#equal: 0 0.002144008783584634 0.9978559912164153&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#equal: 1 0.015842201178506925 0.9841577988214931&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#equal: 2 0.11705891323853292 0.8829410867614671&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#equal: 3 0.8649548767993754 0.13504512320062456&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;not equal:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 0 1 0.002144008783584634 0.015842201178506925&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 0 2 0.002144008783584634 0.11705891323853292&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 0 3 0.002144008783584634 0.8649548767993754&lt;/span&gt;

                &lt;span class=&quot;c&quot;&gt;#not equal: 1 0 0.015842201178506925 0.002144008783584634&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 1 2 0.015842201178506925 0.11705891323853292&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 1 3 0.015842201178506925 0.8649548767993754&lt;/span&gt;

                &lt;span class=&quot;c&quot;&gt;#not equal: 2 0 0.11705891323853292 0.002144008783584634&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 2 1 0.11705891323853292 0.015842201178506925&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 2 3 0.11705891323853292 0.8649548767993754&lt;/span&gt;

                &lt;span class=&quot;c&quot;&gt;#not equal: 3 0 0.8649548767993754 0.002144008783584634&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 3 1 0.8649548767993754 0.015842201178506925&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 3 2 0.8649548767993754 0.11705891323853292&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;#finally resulting in&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[[ 2.13941201e-03 -3.39658185e-05 -2.50975338e-04 -1.85447085e-03]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[-3.39658185e-05  1.55912258e-02 -1.85447085e-03 -1.37027892e-02]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[-2.50975338e-04 -1.85447085e-03  1.03356124e-01 -1.01250678e-01]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[-1.85447085e-03 -1.37027892e-02 -1.01250678e-01  1.16807938e-01]]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The above is the &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/common.py&quot;&gt;softmax grad&lt;/a&gt; code. As you can see it initializes a diagonal matrix that is then populated with right values. On the main diagonal it has the values for case (i=j) and (i!=j) elsewhere. This is illustarted in the picture below.&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/softmaxgrad.png&quot; alt=&quot;Image: figure-1&quot; hight=&quot;120%&quot; width=&quot;120%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As you can see the softmax gradient producers an nxn matrix for input size of n. But what is the relationship between softmax and cross entropy loss function. more importantly, how are the intimate together. This will be illustrated in the next article.&lt;/p&gt;

</description>
        <pubDate>2019-07-11</pubDate>
        <link>http://localhost:4000/articles/2019-05/softmax-and-its-gradient</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2019-05/softmax-and-its-gradient</guid>
        
        <category>Artificial-Intelligence</category>
        
        <category>Deep-Learning</category>
        
        
        <category>article</category>
        
      </item>
    
      <item>
        <title>Introduction: Why another technical blog?</title>
        <description>&lt;p&gt;&lt;strong&gt;Artificial Intelligence especially deep learning employs models which are extremely complex.&lt;/strong&gt; Complexity stems from two major sources.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Firstly&lt;/strong&gt;, Models that have multiple dimensional representation have to be expressed mathematically. Often these models(RNNs, LSTMs, NMTs, BERTs) also have a time dimension that have to be represented as well adding to the complexity.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Secondly&lt;/strong&gt;, the mathematical model has to be translated to code and executed on CPUs/GPUs. When I illustrate these models in later articles, one realizes the importance of looking at the whole stack or at least have an understanding of the whole stack. This enables, in the very least, easier debugging. At best, one understands the model’s execution and behavior and modifications that require to be done to suit our specific use-case(s).&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;In my experience, for most clients I have work for, the single most important reason why models underperform is because the hyperparameters is not understood well and hence most often mis-configured. If, from hardware to the software model and everything in between is understood well, then Deep Learning become fun and magic. &lt;strong&gt;This blog over the next couple of month will try to lower the barrier to entry.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Being in the Optimisation field for quite some time, I have understood the importance of how  stuff works under-the-hood. I have a microarchitecture background, specifically X86 and vector based architectures. Microarchitecture has been my guiding light.  It has helped me solve many complex optimization problems despite some of them being programmed in high level languages like Java or python.&lt;/p&gt;
&lt;p&gt;In this short post, &lt;strong&gt;which has nothing to do with Artificial Intelligence just yet&lt;/strong&gt;, I demonstrate the &lt;strong&gt;importance of understanding stuff under-the-hood&lt;/strong&gt; and &lt;strong&gt;thinking in pictures and then linking it to the code&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;importance-of-understanding-standing-stuff-under-the-hood&quot;&gt;Importance of understanding standing stuff under-the-hood&lt;/h3&gt;
&lt;p&gt;Lets say that one wants understand the underlying assembler for a high level language and the way it executes on a CPU( atleast theoretically ;-)). An easy way to understand this is to look at the usage convention for x86 general purpose registers and do a light reading on the various operations support by the x86 architecture. Once this is known the code becomes extremely clear. Lets understand this with a very simple C program before we do this for a higher level language.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;  &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;multstore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;2 * 3 --&amp;gt; %ld&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mult2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

   &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multstore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
       &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mult2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
       &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Consider the above code where, “main” calls “multstore” and “multstore” calls “mult2”.&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/x86reg.jpg&quot; alt=&quot;Image: figure-1&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Compilers have the above convention in mind for x86 register usage. This is usually done so that software written by different people and often compiled by different compilers can interact with each other.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;callee-saved: If a function A calls function B function B &lt;strong&gt;cannot&lt;/strong&gt; change the values of these registers. This can be done by either not changing values at all or pushing values from these registers to the stack on entry and restoring them on exit.&lt;/li&gt;
    &lt;li&gt;caller-saved: If a function A calls function B then, it means it &lt;strong&gt;can&lt;/strong&gt; be modified by anyone. Caller-saved because if the caller has some local data in these registers then it is caller’s responsibility to save it before making the call.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/assemblercall.jpg&quot; alt=&quot;Image: figure-2&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For most part the above figure should clarify the use of registers in x86. Since the basic idea is clear, the same can be extended for higher level languages.&lt;/p&gt;

&lt;h3 id=&quot;extending-the-general-idea-of-registers-to-higher-level-languagesjava&quot;&gt;Extending the general idea of registers to higher level languages(Java)&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
                  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++;&lt;/span&gt;
              &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
                  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++;&lt;/span&gt;    
                  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
              &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Shape&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;movex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;movey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmpnext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Consider the code above, We have a list of points making up a shape and when “movex” or “movey” function gets called with an  offset it simply moves all the points’ X or Y coordinates by that offset. Now because it is a list of points that make up the shape, moving through that list going to be expensive because the CPU prefethers cannot “guess” the pattern of load and cannot preload to cache. This results in resource stalls , which the another way of saying that the CPU is sitting idle.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;  &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;nanoTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;totalx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;totaly&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;totalpoints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Shape&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shapelist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;totalx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;xsum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;totaly&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;ysum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;totalpoints&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;numpoints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offsetx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totalx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totalpoints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offsety&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totaly&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totalpoints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shapelist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;movex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offsetx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;movey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offsety&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Line 16-21 is where the call is being made to functions “movex” and “movey”. The question is can a profiler pinpoint how the java code executes and the resource stalls that this code is going to encounter. Once this is understood, similar analogy can be extended to more complex code.
Just before we get started a few things to keep in mind.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The profiler being used is &lt;a href=&quot;https://www.oracle.com/technetwork/server-storage/solarisstudio/features/performance-analyzer-2292312.html&quot;&gt;Performance Analyzer&lt;/a&gt; also called &lt;a href=&quot;https://www.oracle.com/technetwork/server-storage/solarisstudio/features/performance-analyzer-2292312.html&quot;&gt;Solaris Analyzer&lt;/a&gt;. Probably one of the best profilers to deep dive into the code at an instruction level. &lt;a href=&quot;https://www.oracle.com/technetwork/server-storage/solarisstudio/features/performance-analyzer-2292312.html&quot;&gt;Performance Analyzers&lt;/a&gt; would require a post in itself.&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;https://www.oracle.com/technetwork/server-storage/solarisstudio/features/performance-analyzer-2292312.html&quot;&gt;performance analyzer&lt;/a&gt; for this run has be configured to capture resource stalls for the above code.&lt;/li&gt;
    &lt;li&gt;Despite Java being an interpreted language, in practice it is opmtimized on-the-fly by a &lt;a href=&quot;https://en.wikipedia.org/wiki/Just-in-time_compilation&quot;&gt;JIT compiler&lt;/a&gt;. These are usually profile guided optimizations and topic for another day. The code shown by &lt;a href=&quot;https://www.oracle.com/technetwork/server-storage/solarisstudio/features/performance-analyzer-2292312.html&quot;&gt;Performance Analyzers&lt;/a&gt; is &lt;a href=&quot;https://en.wikipedia.org/wiki/Just-in-time_compilation&quot;&gt;JIT compiler&lt;/a&gt; generated. Typically we get this profile when the code has run for some time for it to be JIT compiled.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;code-walk-through-using-solaris-analyzer&quot;&gt;Code walk through using Solaris Analyzer&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Measuring CPU cycles for the code&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol1.jpe&quot; alt=&quot;Image: figure-3:CPU Cycles for the code in listing 1 and 2.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3:CPU Cycles for the code in listing 1 and 2.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-1&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol2.jpeg&quot; alt=&quot;Image: figure-4:Bottom of the screen is the size of the object using &amp;lt;a href='https://openjdk.java.net/projects/code-tools/jmh/' &amp;gt;JMH&amp;lt;/a&amp;gt;. &quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4:Bottom of the screen is the size of the object using &lt;a href=&quot;https://openjdk.java.net/projects/code-tools/jmh/&quot;&gt;JMH&lt;/a&gt;. &lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-2&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol3.jpeg&quot; alt=&quot;Image: figure-5&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-3&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol4.jpeg&quot; alt=&quot;Image: figure-6&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-4&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol5.jpeg&quot; alt=&quot;Image: figure-7:This is the big culprit, loading the next pointer. And this will show up as major resource consumer in figure-17.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7:This is the big culprit, loading the next pointer. And this will show up as major resource consumer in figure-17.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-5&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;a href=&quot;https://medium.com/software-under-the-hood/under-the-hood-java-peak-safepoints-dd45af07d766&quot;&gt;
        &lt;img src=&quot;/img/introduction/sol6.jpeg&quot; alt=&quot;Image: figure-8: A &amp;lt;a href='https://medium.com/software-under-the-hood/under-the-hood-java-peak-safepoints-dd45af07d766'&amp;gt;JVM safepoint&amp;lt;/a&amp;gt; is a range of execution where the state of the executing thread is well described.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    &lt;/a&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-8: A &lt;a href=&quot;https://medium.com/software-under-the-hood/under-the-hood-java-peak-safepoints-dd45af07d766&quot;&gt;JVM safepoint&lt;/a&gt; is a range of execution where the state of the executing thread is well described.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-6&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol7.jpeg&quot; alt=&quot;Image: figure-9&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-9&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-7&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol8.jpeg&quot; alt=&quot;Image: figure-10&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-10&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-8&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol9.jpeg&quot; alt=&quot;Image: figure-11&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-11&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-9&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol10.jpeg&quot; alt=&quot;Image: figure-12&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-12&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-10&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol11.jpeg&quot; alt=&quot;Image: figure-13&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-13&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-11&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol12.jpeg&quot; alt=&quot;Image: figure-14:This is the big culprit, loading the next pointer except it is for y. This too will show up as major resource consumer in figure-17.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-14:This is the big culprit, loading the next pointer except it is for y. This too will show up as major resource consumer in figure-17.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-12&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol13.jpeg&quot; alt=&quot;Image: figure-15&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-15&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-13&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol14.jpeg&quot; alt=&quot;Image: figure-16&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-16&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Walk through:Step-14&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/introduction/sol15.jpeg&quot; alt=&quot;Image: figure-17&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-17&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;In summary then, the figure-17 should clarify the resource stalls exhibited by the code. It is time consuming to take first principles approach to undertand deep technical concepts but ultimately it brings great joy. I would like to bring similar undetsanding to concepts in general and Deep Learning Concepts in particular. In fact, I have seen Neural Machine Translation Based systems grossly underperform and, it was simply because most of the hyperparameters were not understood at all. &lt;strong&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;Deep-Breathe&lt;/a&gt;&lt;/strong&gt; is a complete and pure python implementation of these models, specially but not limited to &lt;strong&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/manualmachinetranslation.py&quot;&gt;Neural Machine Translator&lt;/a&gt;&lt;/strong&gt; . &lt;strong&gt;In the next few articles I will go into the details of their inner workings. And in this blog I will focus more on Pictures and Code &lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>2019-07-11</pubDate>
        <link>http://localhost:4000/articles/2019-05/introduction</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2019-05/introduction</guid>
        
        <category>Introduction</category>
        
        
        <category>article</category>
        
      </item>
    
  </channel>
</rss>
