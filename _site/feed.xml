<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slowbreathing</title>
    <description>Programming is more than just typing.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>2019-05-05</pubDate>
    <lastBuildDate>Sun, 05 May 2019 08:33:12 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Softmax and Cross-entropy</title>
        <description>&lt;p&gt;Cross entropy is a loss function that is defined as &lt;script type=&quot;math/tex&quot;&gt;\Large E = -y .log ({\hat{Y}})&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\Large E&lt;/script&gt;, is difined as error, &lt;script type=&quot;math/tex&quot;&gt;\Large y&lt;/script&gt; is the label and &lt;script type=&quot;math/tex&quot;&gt;\Large \hat{Y}&lt;/script&gt; is defined as &lt;script type=&quot;math/tex&quot;&gt;\Large softmax_j (logits)&lt;/script&gt; and logits are weighted sum. One of the reason to choose cross entropy alongside softmx is that because softmax has an exponential element inside it the, an element log provides for convex cost function. This is similar to logistic regression which uses sigmoid. Mathematically expressed,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Large E = -y .log ({\hat{Y}})&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Large \hat{Y}=softmax_j (logits)&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;crossentropy-loss-derivative&quot;&gt;CrossEntropy Loss Derivative&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = \frac{\partial {E}}{\partial {\hat{Y}}}.\frac{\partial {\hat{Y}}}{\partial {logits}}&lt;/script&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;eq-2&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\frac{\partial {\hat{Y}}}{\partial {logits}} \:\:\:\: eq(2)&lt;/script&gt;&lt;br /&gt;
For calculating &lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial {\hat{Y}}}{\partial {logits}}&lt;/script&gt; we need to reference &lt;a href=&quot;softmax-and-its-gradient/#eq-1&quot;&gt;eq-1&lt;/a&gt;. Combining &lt;a href=&quot;softmax-and-its-gradient/#eq-1&quot;&gt;eq-1&lt;/a&gt; and &lt;a href=&quot;softmax-and-cross-entropy#eq-2&quot;&gt;eq-2&lt;/a&gt; which is softmax.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\begin{Bmatrix}
softmax_i(1-softmax_j) &amp; i= j \\
{-softmax_j}.{softmax_i} &amp; i\neq j
\end{Bmatrix} %]]&gt;&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\begin{Bmatrix}
softmax_i(1-softmax_j) &amp; i= j \\
{-softmax_j}.{softmax_i} &amp; i\neq j
\end{Bmatrix} %]]&gt;&lt;/script&gt;&lt;/p&gt;

</description>
        <pubDate>2019-05-04</pubDate>
        <link>http://localhost:4000/articles/2019-05/softmax-and-cross-entropy</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2019-05/softmax-and-cross-entropy</guid>
        
        
        <category>article</category>
        
      </item>
    
      <item>
        <title>Softmax and its Gradient</title>
        <description>&lt;p&gt;From the perspective of peep neural network, softmax is one the most important activation function, maybe the most important. I had trouble understanding it in the beginning, especially its why its chosen, its gradient, its relationship with cross entropy loss and the combined gradient.&lt;br /&gt;
There are many softmax resources available in the internet. Among the many that are availble, I found the link &lt;a href=&quot;https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/&quot;&gt;here&lt;/a&gt; to be the most complete. In this article, I “dumb” it down further and add code to the theory. Code when associated with theory makes the explanation very precise.&lt;/p&gt;

&lt;p&gt;Softmax is essentially a vector function. It takes n inputs and produces and n outputs.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large softmax(a)=\begin{bmatrix}
a_1\\
a_2\\
\cdots\\
a_N
\end{bmatrix}\rightarrow \begin{bmatrix}
S_1\\
S_2\\
\cdots\\
S_N
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;And the actual per-element formula is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large softmax_j = \frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}&lt;/script&gt;

&lt;p&gt;As one can see the output function can only be positive because of the exponential and the values range between 0 and 1. Also,  as the value  appears in the denominator summed up with other positive numbers.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#row represents num classes but they may be real numbers&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#So the shape of input is important&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#([[1, 3, 5, 7],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#  [1,-9, 4, 8]]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;#softmax will be for each of the 2 rows&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[[2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[8.94679461e-04 4.06183847e-08 1.79701173e-02 9.81135163e-01]]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#respectively But if the input is Tranposed clearly the answer&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#will be wrong.&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;#That needs to be converted to probability&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#column represents the vocabulary size.&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predsl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#print(&quot;inputs:&quot;,inputs)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;predsl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predsl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Above is a &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/common.py&quot;&gt;softmax&lt;/a&gt; implementation.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;softmax:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#prints out&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[[2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[8.94679461e-04 4.06183847e-08 1.79701173e-02 9.81135163e-01]]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Above is a &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/softmaxtest.py&quot;&gt;softmaxtest&lt;/a&gt; implementation.&lt;/p&gt;

&lt;p&gt;The softmax function is very similar to the Logistic regression cost function. The only difference being that the sigmoid makes the output binary interpretable whereas, softmax’s output can be interpreted as a multiway shootout. With the the above two rows individually summing up to one.&lt;/p&gt;

&lt;h3 id=&quot;softmax-derivative&quot;&gt;Softmax Derivative&lt;/h3&gt;
&lt;p&gt;Before diving into computing the derivative of softmax, let’s start with some preliminaries from vector calculus.&lt;/p&gt;

&lt;p&gt;Softmax is fundamentally a vector function. It takes a vector as input and produces a vector as output; in other words, it has multiple inputs and multiple outputs. Therefore, we cannot just ask for “the derivative of softmax”; We should instead specify:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Which component (output element) of softmax we’re seeking to find the derivative of.&lt;/li&gt;
  &lt;li&gt;Since softmax has multiple inputs, with respect to which input element the partial derivative is computed.
This is exactly why the notation of vector calculus was developed. What we’re looking for is the partial derivatives:&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large \frac{\partial softmax_i}{\partial a_j}&lt;/script&gt;

&lt;p&gt;This is the partial derivative of the i-th output w.r.t. the j-th input. A shorter way to write it that we’ll be using going forward is:
Since softmax is a  function, the most general derivative we compute for it is the Jacobian matrix:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Large Dsoftmax=\begin{bmatrix}
D_1 softmax_1\:\:\:\:\:  \cdots\:\:\:\:\: D_N softmax_1 \\
\vdots\:\:\:\:\: \ddots\:\:\:\:\: \vdots \\
D_1 softmax_N\:\:\:\:\: \cdots\:\:\:\:\: D_N softmax_N
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;Let’s compute &lt;script type=&quot;math/tex&quot;&gt;D_jsoftmax_i&lt;/script&gt; for arbitrary i and j:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\large D_jsoftmax_i=\Large \:\:\frac{\partial softmax_i}{\partial a_j}\:\:=\frac{\partial softmax_i}{\partial a_j} = \frac{\partial \frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}}{\partial a_j}&lt;/script&gt;&lt;br /&gt;
Using the quotient rule &lt;script type=&quot;math/tex&quot;&gt;\Large f(x) = \frac{g(x)}{h(x)}&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;\Large f'(x) = \frac{g'(x)h(x) - h'(x)g(x)}{h(x)^2}&lt;/script&gt; &lt;br /&gt;
in our case &lt;script type=&quot;math/tex&quot;&gt;\Large g_i = e^{a_i}&lt;/script&gt;&lt;br /&gt;
                   &lt;script type=&quot;math/tex&quot;&gt;\Large h_i = \sum_{k=1}^{N} e^{a_k}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Note that no matter which &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; we compute the derivative of &lt;script type=&quot;math/tex&quot;&gt;h_i&lt;/script&gt; for, the answer will always be &lt;script type=&quot;math/tex&quot;&gt;e^{a_j}&lt;/script&gt;. This is not the case for &lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt;, howewer. The derivative of &lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt; w.r.t. &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;e^{a_j}&lt;/script&gt; only if i=j, because only then &lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt; has &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; anywhere in it. Otherwise, the derivative is 0.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Going back to our &lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial softmax_i}{\partial a_j}&lt;/script&gt; we’ll start with the &lt;script type=&quot;math/tex&quot;&gt;\Large i=j&lt;/script&gt; case. &lt;br /&gt;
Then, using the quotient rule we have:
&lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial\frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}}{\partial a_j} = \frac{e^{a_i}.\sum-e^{a_j}.e^{a_i}}{\sum^2}&lt;/script&gt;&lt;br /&gt;
                         &lt;script type=&quot;math/tex&quot;&gt;\Large = \frac{e^{a_i}}{\sum}.\frac{\sum-e^{a_j}}{\sum}&lt;/script&gt;&lt;br /&gt;
                         &lt;script type=&quot;math/tex&quot;&gt;\Large = softmax_i(1-softmax_j)&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;similarly for &lt;script type=&quot;math/tex&quot;&gt;\Large i \neq j&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Large \frac{\partial\frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}}{\partial a_j} = \frac{0- e^{a_j}e^{a_i}}{\sum^2}&lt;/script&gt;&lt;br /&gt;
                         &lt;script type=&quot;math/tex&quot;&gt;\Large = \frac{- e^{a_j}}{\sum}.\frac{e^{a_i}}{\sum}&lt;/script&gt;&lt;br /&gt;
                         &lt;script type=&quot;math/tex&quot;&gt;\Large = {-softmax_j}.{softmax_i}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To summarize:&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;eq-1&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\Large D_jsoftmax_i=\begin{Bmatrix}
softmax_i(1-softmax_j) &amp; i= j \\
{-softmax_j}.{softmax_i} &amp; i\neq j
\end{Bmatrix} \:\:\:\: eq(1) %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_softmax_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Below is the softmax value for [1, 3, 5, 7]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# [2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# initialize the 2-D jacobian matrix.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;equal:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#equal: 0 0.002144008783584634 0.9978559912164153&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#equal: 1 0.015842201178506925 0.9841577988214931&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#equal: 2 0.11705891323853292 0.8829410867614671&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#equal: 3 0.8649548767993754 0.13504512320062456&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;not equal:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 0 1 0.002144008783584634 0.015842201178506925&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 0 2 0.002144008783584634 0.11705891323853292&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 0 3 0.002144008783584634 0.8649548767993754&lt;/span&gt;

                &lt;span class=&quot;c&quot;&gt;#not equal: 1 0 0.015842201178506925 0.002144008783584634&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 1 2 0.015842201178506925 0.11705891323853292&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 1 3 0.015842201178506925 0.8649548767993754&lt;/span&gt;

                &lt;span class=&quot;c&quot;&gt;#not equal: 2 0 0.11705891323853292 0.002144008783584634&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 2 1 0.11705891323853292 0.015842201178506925&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 2 3 0.11705891323853292 0.8649548767993754&lt;/span&gt;

                &lt;span class=&quot;c&quot;&gt;#not equal: 3 0 0.8649548767993754 0.002144008783584634&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 3 1 0.8649548767993754 0.015842201178506925&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#not equal: 3 2 0.8649548767993754 0.11705891323853292&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;#finally resulting in&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[[ 2.13941201e-03 -3.39658185e-05 -2.50975338e-04 -1.85447085e-03]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[-3.39658185e-05  1.55912258e-02 -1.85447085e-03 -1.37027892e-02]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[-2.50975338e-04 -1.85447085e-03  1.03356124e-01 -1.01250678e-01]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#[-1.85447085e-03 -1.37027892e-02 -1.01250678e-01  1.16807938e-01]]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jacobian_m&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The above is the &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/common.py&quot;&gt;softmax grad&lt;/a&gt; code. As you can see it initializes a diagonal matrix that is then populated with right values. On the main diagonal it has the values for case (i=j) and (i!=j) elsewhere. this is illustarted in the picture below.
&lt;img src=&quot;http://localhost:4000/img/softmaxgrad.png&quot; alt=&quot;softmax grad&quot; hight=&quot;1080%&quot; width=&quot;1920%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see the softmax gradient producers an nxn matrix for input size of n. But what is the relationship between softmax and cross entropy loss function. more importantly, how are the intimate together. This will be illustrated in the next article.&lt;/p&gt;

</description>
        <pubDate>2019-05-04</pubDate>
        <link>http://localhost:4000/articles/2019-05/softmax-and-its-gradient</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2019-05/softmax-and-its-gradient</guid>
        
        
        <category>article</category>
        
      </item>
    
  </channel>
</rss>
