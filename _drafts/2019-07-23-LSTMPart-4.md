---
layout: post
title:  "RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs"
excerpt: "In this multi-part series(<a href='/articles/2019-07/LSTMPart-1'>part-1</a>,<a href='/articles/2019-07/LSTMPart-2'>part-2</a>,<a href='/articles/2019-07/LSTMPart-3'>part-3</a>) we look into composing LSTM into multiple higher layers and its directionalily. <strong>Multiple layers</strong> though compute intensive are great accuracy and so is <strong>bidirectional connections.</strong> More importantly a solid understanding of the above mentioned paves the way for concepts like <strong>Highway connections</strong>, <strong>Residual Connections</strong>, <strong>Pointer networks</strong>, <strong>Encoder-Decoder</strong> Architectures and so forth in future article. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>"
mathjax: true
date:   2019-07-22 15:07:19
categories: [article]
tags: Artificial-Intelligence Deep-Learning LSTM RNN Sequence-Modelling
comments: true
---

### Introduction
In this [multi-part series] (<a href='/articles/2019-07/LSTMPart-1'>part-1</a>,<a href='/articles/2019-07/LSTMPart-2'>part-2</a>,<a href='/articles/2019-07/LSTMPart-3'>part-3</a>) we look into composing LSTM into multiple higher layers and its directionalily. <strong>Multiple layers</strong> though compute intensive are great accuracy and so is <strong>bidirectional connections.</strong> More importantly a solid understanding of the above mentioned paves the way for concepts like <strong>Highway connections</strong>, <strong>Residual Connections</strong>, <strong>Pointer networks</strong>, <strong>Encoder-Decoder</strong> Architectures and so forth in future article. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>

### What this article is not about?
> * This article will not talk about the conceptual model of LSTM, on which there are some great existing material [here][lstm-1] and [here][lstm-2] in thye order of difficulty.
> * This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome post by Andrej if somewhat [difficult post][lstm-3].
> * This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts [here][lstm-4] and [here][lstm-5] in the order of difficulty     

### What this article is about?
> * Using the [first principles][first principles] we picturize [MultiRNNCell] and [Bidirectional RNNs] and their backward propagation.
> * Then we use a heady mixture of intuition, Logic, and programming to prove mathematical results.   

### Context
This is the same example and the context is the same as described in [part-1]. <strong>The focus however, this time is on [MultiRNNCell] and [Bidirectional RNNs]</strong>

### Multi layer RNNs
1. [Recurent depth]
2. [Feed Forward depth]
3. [Multi layer RNNs Forward pass]
4. [Multi layer RNNs backward propagation]

# <a name="Recurent depth"></a>
#### Recurent depth
A quick recap, LSTM encapsulates the internal cell logic. There are many variations to Cell logic, VanillaRNN, LSTMs, GRUs etc. What we have seen so far is they can be fed sequiential time series kind of data. We feed in the sequence, compare with the labels, calculate errors back propagate and adjust the weights. The length of the fed in sequence is informally called recurrent depth.

{%
    include image.html
    src="/img/rnn/multilayercell.jpg"
    caption="figure-1: <strong>Recurrent depth</strong>=3"
    hight="110%"
    width="110%"
%}
Figure-1 shows the recurrent depth. Formally, <strong>Recurrent depth is the Longest path between same hidden state in successive timesteps</strong>. The recurrent depth is amply clear.

# <a name="Feed Forward depth"></a>
#### Feed Forward depth
The topic of this article is Feed-forward depth, more akin to the depth we have in vanilla neural networks. It is the depth of a network that is generally attributed to the success of deep learning as a technique. There are downsides too, too much compute budget for one if done indiscrimately. That being said, well look at the inner workings of Multiple layers and how we set it up in code.  

{%
    include image.html
    src="/img/rnn/multilayercell2.jpg"
    caption="figure-2: <strong>Feed forward depth</strong>=3"
    hight="110%"
    width="110%"
%}
Figure-2 shows the feed forward depth. Formally longest path between an input and output at the same timestep.

# <a name="Multi layer RNNs Forward pass"></a>
#### Multi layer RNNs Forward pass

For most part this straight forward as you have already seen in the [previous article][part-3] in this series. The only difference being that there are multiple cells(2 in this example) now and how the state flows forward and gradient flows backwards.

> * <strong>[MultiRNNCell]</strong> forward pass summary.
{%
    include image.html
    src="/img/rnn/multilayercell7.jpg"
    caption="figure-3: <strong>Multi layer RNNs Forward pass summary</strong>"  
    hight="110%"
    width="110%"
%}

> * Multi layer RNNs Forward pass
> * Notice the state being passed(yellow) from first layer to the second.
> * The softmax and cross_entropy_loss is done on the second layer output expectedly.
{%
    include image.html
    src="/img/rnn/multilayercell3.jpg"
    caption="figure-4: <strong>Multi layer RNNs Forward pass</strong>"  
    hight="110%"
    width="110%"
%}

> * Complete code of <strong>[MultiRNNCell]</strong> from [DEEP-Breathe] helps in looking at the internals
{%
    include image.html
    src="/img/rnn/multilayercell4.jpg"
    caption="figure-5: <strong>DHt</strong>"
    hight="110%"
    width="110%"
%}

> * [MultiRNNCell] <strong>outputs</strong> returned
{%
    include image.html
    src="/img/rnn/multilayercell5.jpg"
    caption="figure-6: <strong>outputs</strong> returned."
    hight="110%"
    width="110%"
%}

> * [MultiRNNCell] <strong>states</strong> returned
{%
    include image.html
    src="/img/rnn/multilayercell6.jpg"
    caption="figure-7: <strong>states</strong> returned."
    hight="110%"
    width="110%"
%}
Also when using [Deep-Breathe] you could enable forward or backward logging for MultiRNNCell like so
{% highlight python %}
  cell= MultiRNNCell([cell1, cell2],debug=True,backpassdebug=True)
  Listing-1
{% endhighlight %}

# <a name="Multi layer RNNs backward propagation"></a>
#### Multi layer RNNs backward propagation
Again, for most part, this is almost identical to standard LSTM Backward Propagation which we went through in detail in [part-3]. So if you have got a good hang of it, only a few things change. But before we go on, refresh [DHX] and the complete [section][Backward propagation through LSTMCell(Step-2)] before you resume here. Here what we have to calculate.

> * [MultiRNNCell] Backward propagation summary.
{%
    include image.html
    src="/img/rnn/multilayercell8.jpg"
    caption="figure-8: MultiRNNCell Backward propagation summary."
    hight="110%"
    width="110%"
%}

<strong>I'll reproduce DHX here just to set the context and note down the changes.</strong>
###### DHX
> * <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>.
> * Complete listing can be found at [code-15]
{%
    include image.html
    src="/img/rnn/lstmback26.jpg"
    caption="figure-9: <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>."
    hight="110%"
    width="110%"
%}

> * Many models like Neural Machine Translators(NMT), Bidirectional Encoder Representation from Transformers(BERT) use word embeddings as their inputs(Xs) which often times need learrning and that is where we need <strong>dxt</strong>
{%
    include image.html
    src="/img/rnn/lstmback27.jpg"
    caption="figure-30: <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>"
    hight="110%"
    width="110%"
%}


### Summary
There you go, a complete documentation of LSTMCell's backward propagation. Also when using [Deep-Breathe] you could enable forward or backward logging using
{% highlight python %}
  cell = LSTMCell(n_hidden,debug=False,backpassdebug=True)
  out_l = Dense(10,kernel_initializer=init_ops.Constant(out_weights),bias_initializer=init_ops.Constant(out_biases),backpassdebug=True,debug=False)


  Listing-1
{% endhighlight %}
What we went through in this article is "hell". But if you have come so far then the good news is, <strong>it can only get easier</strong>. When we do use LSTMs in more complex architecture like Neural Machine Translators<strong>(NMT)</strong>, Bidirectional Encoder Representation from Transformers<strong>(BERT)</strong>, or a Differentiable Neural Computers(DNC) <strong>it will be a walk in a blooming lovely park</strong>. Well alomost!! ;-) .In a later article well demystify the multilayer LSTMcells and Bidirection LSTMCells.


[Recurent depth]: LSTMPart-4#Recurent depth
[Feed Forward depth]: LSTMPart-4#Feed Forward depth
[Multi layer RNNs Forward pass]: LSTMPart-4#Multi layer RNNs Forward pass
[Multi layer RNNs backward propagation]: LSTMPart-4#Multi layer RNNs backward propagation
[Backward propagation through LSTMCell(Step-2)]: LSTMPart-3#Backward propagation through LSTMCell(Step-2)

[DHt]: LSTMPart-3#DHt
[DOt]: LSTMPart-3#DOt
[DCt]: LSTMPart-3#DCt
[DCprojt]: LSTMPart-3#DCprojt
[DFt]: LSTMPart-3#DFt
[DIt]: LSTMPart-3#DIt
[DHX]: LSTMPart-3#DHX
[DCt_recur]: LSTMPart-3#DCt_recur
[figure-6]: LSTMPart-3#figure-6


[part-1]: /articles/2019-07/LSTMPart-1
[part-2]: /articles/2019-07/LSTMPart-2
[part-3]: /articles/2019-07/LSTMPart-3
[part-4]: /articles/2019-07/LSTMPart-4
[multi-part series]: /tags/#LSTM
[first principles]: https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0
[Deep-Breathe]: https://github.com/slowbreathing/Deep-Breathe
[LayerNormBasicLSTMCell]: https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell
[lstm-1]: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
[lstm-2]: https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
[lstm-3]: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
[lstm-4]: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html
[lstm-5]: https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577

[eq-1]: /articles/2019-05/softmax-and-its-gradient#eq-1
[eq-2]: /articles/2019-05/softmax-and-cross-entropy#eq-2
[eq-3]: /articles/2019-05/softmax-and-cross-entropy#eq-3

[code-1]: https://github.com/slowbreathing/Deep-Breathe/blob/3fc68956eeaa082b59af2d181ff66e3932f49415/org/mk/training/dl/core.py#L157-L170
[code-2]: https://github.com/slowbreathing/Deep-Breathe/blob/3fc68956eeaa082b59af2d181ff66e3932f49415/org/mk/training/dl/rnn.py#L444
[code-3]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L445
[code-4]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L492-L499
[code-5]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L295-L299
[code-6]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L328
[code-7]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L329-L331
[code-8]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L309
[code-9]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L323
[code-10]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L324-L326
[code-11]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L313
[code-12]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L314-L316
[code-13]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L318
[code-14]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L320-L321
[code-15]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344
[code-16]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L342

[MultiRNNCell]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193
[Bidirectional RNNs]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416
[softmax]: /articles/2019-05/softmax-and-its-gradient
[cross_entropy_loss]: /articles/2019-05/softmax-and-cross-entropy
[Tensorflow]: https://www.tensorflow.org/
[Listing-1]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/tfwordslstm.py
[Listing-2]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMainGraph.py
[Listing-3]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMain.py
[scr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L13
[scr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L15
[scr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L14
[pygr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn.py#L75-L76
[pygr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/common.py#L241-L255
[pygr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn_cell.py#L79-L81
[pygr-4]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/core.py#L107-L112
